–ü—Ä–∏–º–µ—Ä—ã —Ü–µ–ø–æ—á–µ–∫
===============

–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Evolution LangChain —Å LangChain —Ü–µ–ø–æ—á–∫–∞–º–∏.

–ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ø–æ—á–∫–∞
---------------

–ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ø–æ—á–∫–∏:

.. code-block:: python

   from evolution_langchain import EvolutionInference
   from langchain.chains import LLMChain
   from langchain.prompts import PromptTemplate

   # –°–æ–∑–¥–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞
   template = "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}"
   prompt = PromptTemplate(template=template, input_variables=["question"])

   # –°–æ–∑–¥–∞–Ω–∏–µ LLM
   llm = EvolutionInference(
       model="your-model-name",
       key_id="your-key-id",
       secret="your-secret",
       base_url="https://your-api-endpoint.com/v1"
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
   chain = LLMChain(llm=llm, prompt=prompt)

   # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
   result = chain.run("–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?")
   print(result)

–¶–µ–ø–æ—á–∫–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
---------------------------------

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:

.. code-block:: python

   from evolution_langchain import EvolutionInference
   from langchain.chains import LLMChain
   from langchain.prompts import PromptTemplate

   # –®–∞–±–ª–æ–Ω —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
   template = """
   –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ {topic}. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–¥—Ä–æ–±–Ω–æ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ.
   
   –í–æ–ø—Ä–æ—Å: {question}
   
   –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {difficulty}
   
   –û—Ç–≤–µ—Ç:"""

   prompt = PromptTemplate(
       template=template,
       input_variables=["topic", "question", "difficulty"]
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ LLM
   llm = EvolutionInference(
       model="your-model-name",
       key_id="your-key-id",
       secret="your-secret",
       base_url="https://your-api-endpoint.com/v1"
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
   chain = LLMChain(llm=llm, prompt=prompt)

   # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
   result = chain.run({
       "topic": "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
       "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏?",
       "difficulty": "–Ω–∞—á–∞–ª—å–Ω—ã–π"
   })
   print(result)

–¶–µ–ø–æ—á–∫–∞ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
-------------------------

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —à–∞–±–ª–æ–Ω–∞—Ö:

.. code-block:: python

   from evolution_langchain import EvolutionInference
   from langchain.chains import LLMChain
   from langchain.prompts import PromptTemplate

   # –®–∞–±–ª–æ–Ω —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
   template = """
   # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
   
   –¢—ã - {role}. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: {task}
   
   ## –ö–æ–Ω—Ç–µ–∫—Å—Ç
   {context}
   
   ## –ó–∞–ø—Ä–æ—Å
   {request}
   
   ## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
   - –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {length}
   - –°—Ç–∏–ª—å: {style}
   - –Ø–∑—ã–∫: {language}
   
   ## –û—Ç–≤–µ—Ç
   """

   prompt = PromptTemplate(
       template=template,
       input_variables=["role", "task", "context", "request", "length", "style", "language"]
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ LLM
   llm = EvolutionInference(
       model="your-model-name",
       key_id="your-key-id",
       secret="your-secret",
       base_url="https://your-api-endpoint.com/v1"
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
   chain = LLMChain(llm=llm, prompt=prompt)

   # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
   result = chain.run({
       "role": "—ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é",
       "task": "–æ–±—ä—è—Å–Ω–∏—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏—é",
       "context": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–∑—É—á–∞–µ—Ç Python",
       "request": "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ—É–Ω–∫—Ü–∏–∏?",
       "length": "–∫—Ä–∞—Ç–∫–æ",
       "style": "–ø—Ä–æ—Å—Ç–æ–π",
       "language": "—Ä—É—Å—Å–∫–∏–π"
   })
   print(result)

–¶–µ–ø–æ—á–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
--------------------

–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

.. code-block:: python

   from evolution_langchain import EvolutionInference
   from langchain.chains import LLMChain
   from langchain.prompts import PromptTemplate
   from langchain.output_parsers import PydanticOutputParser
   from pydantic import BaseModel, Field
   from typing import List

   # –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–∞
   class CodeExplanation(BaseModel):
       concept: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏")
       explanation: str = Field(description="–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏")
       examples: List[str] = Field(description="–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
       difficulty: str = Field(description="–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")

   # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞
   parser = PydanticOutputParser(pydantic_object=CodeExplanation)

   # –®–∞–±–ª–æ–Ω —Å –ø–∞—Ä—Å–µ—Ä–æ–º
   template = """
   –û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:
   
   {format_instructions}
   
   –ö–æ–Ω—Ü–µ–ø—Ü–∏—è: {concept}
   
   –û–±—ä—è—Å–Ω–µ–Ω–∏–µ:"""

   prompt = PromptTemplate(
       template=template,
       input_variables=["concept"],
       partial_variables={"format_instructions": parser.get_format_instructions()}
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ LLM
   llm = EvolutionInference(
       model="your-model-name",
       key_id="your-key-id",
       secret="your-secret",
       base_url="https://your-api-endpoint.com/v1"
   )

   # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
   chain = LLMChain(llm=llm, prompt=prompt)

   # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º
   result = chain.run("—Ñ—É–Ω–∫—Ü–∏–∏")
   parsed_result = parser.parse(result)
   print(f"–ö–æ–Ω—Ü–µ–ø—Ü–∏—è: {parsed_result.concept}")
   print(f"–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {parsed_result.explanation}")
   print(f"–ü—Ä–∏–º–µ—Ä—ã: {parsed_result.examples}")
   print(f"–°–ª–æ–∂–Ω–æ—Å—Ç—å: {parsed_result.difficulty}")

–¶–µ–ø–æ—á–∫–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
---------------------------

–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫:

.. code-block:: python

   from evolution_langchain import EvolutionInference
   from langchain.chains import LLMChain
   from langchain.prompts import PromptTemplate
   import time

   def create_chain_with_retry():
       # –°–æ–∑–¥–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞
       template = "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}"
       prompt = PromptTemplate(template=template, input_variables=["question"])

       # –°–æ–∑–¥–∞–Ω–∏–µ LLM
       llm = EvolutionInference(
           model="your-model-name",
           key_id="your-key-id",
           secret="your-secret",
           base_url="https://your-api-endpoint.com/v1"
       )

       # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
       chain = LLMChain(llm=llm, prompt=prompt)
       return chain

   def run_chain_with_retry(chain, question, max_retries=3):
       for attempt in range(max_retries):
           try:
               result = chain.run(question)
               return result
           except Exception as e:
               print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
               if attempt < max_retries - 1:
                   time.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
               else:
                   raise e

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   chain = create_chain_with_retry()
   
   try:
       result = run_chain_with_retry(chain, "–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
       print(result)
   except Exception as e:
       print(f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å: {e}")

–ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä
-------------

–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ü–µ–ø–æ—á–µ–∫:

.. code-block:: python

   import os
   from evolution_langchain import EvolutionInference
   from langchain.chains import LLMChain
   from langchain.prompts import PromptTemplate

   def main():
       print("üöÄ Evolution LangChain - –ü—Ä–∏–º–µ—Ä—ã —Ü–µ–ø–æ—á–µ–∫")
       print("=" * 50)

       # –°–æ–∑–¥–∞–Ω–∏–µ LLM
       llm = EvolutionInference(
           model=os.getenv("EVOLUTION_MODEL", "your-model-name"),
           key_id=os.getenv("EVOLUTION_KEY_ID", "your-key-id"),
           secret=os.getenv("EVOLUTION_SECRET", "your-secret"),
           base_url=os.getenv("EVOLUTION_BASE_URL", "https://your-api-endpoint.com/v1")
       )

       print("‚úÖ LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
       print()

       # 1. –ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ø–æ—á–∫–∞
       print("1. –ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ø–æ—á–∫–∞:")
       template1 = "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}"
       prompt1 = PromptTemplate(template=template1, input_variables=["question"])
       chain1 = LLMChain(llm=llm, prompt=prompt1)

       try:
           result1 = chain1.run("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
           print(f"–û—Ç–≤–µ—Ç: {result1}")
       except Exception as e:
           print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
       print()

       # 2. –¶–µ–ø–æ—á–∫–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
       print("2. –¶–µ–ø–æ—á–∫–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏:")
       template2 = """
       –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ {topic}. 
       –û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é '{concept}' –¥–ª—è —É—Ä–æ–≤–Ω—è {level}.
       
       –û–±—ä—è—Å–Ω–µ–Ω–∏–µ:"""

       prompt2 = PromptTemplate(
           template=template2,
           input_variables=["topic", "concept", "level"]
       )
       chain2 = LLMChain(llm=llm, prompt=prompt2)

       try:
           result2 = chain2.run({
               "topic": "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
               "concept": "—Ñ—É–Ω–∫—Ü–∏–∏",
               "level": "–Ω–æ–≤–∏—á–æ–∫"
           })
           print(f"–û—Ç–≤–µ—Ç: {result2}")
       except Exception as e:
           print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
       print()

       # 3. –¶–µ–ø–æ—á–∫–∞ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
       print("3. –¶–µ–ø–æ—á–∫–∞ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º:")
       template3 = """
       # –ó–∞–¥–∞—á–∞
       –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏.
       
       ## –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è: {technology}
       ## –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: {purpose}
       
       ## –û–ø–∏—Å–∞–Ω–∏–µ
       """

       prompt3 = PromptTemplate(
           template=template3,
           input_variables=["technology", "purpose"]
       )
       chain3 = LLMChain(llm=llm, prompt=prompt3)

       try:
           result3 = chain3.run({
               "technology": "Docker",
               "purpose": "–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π"
           })
           print(f"–û—Ç–≤–µ—Ç: {result3}")
       except Exception as e:
           print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
       print()

       print("üéâ –ü—Ä–∏–º–µ—Ä—ã —Ü–µ–ø–æ—á–µ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")

   if __name__ == "__main__":
       main()

–ß—Ç–æ –¥–∞–ª—å—à–µ?
-----------

- –ò–∑—É—á–∏—Ç–µ :doc:`advanced` –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
- –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ :doc:`../guide/langchain-integration` –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ :doc:`../api/evolution-inference` –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏ –ø–æ API 