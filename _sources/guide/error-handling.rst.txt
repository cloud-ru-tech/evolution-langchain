–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
================

–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—à–∏–±–æ–∫ –≤ Evolution LangChain.

–¢–∏–ø—ã –æ—à–∏–±–æ–∫
-----------

ValueError
~~~~~~~~~~

–í–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –Ω–µ–≤–µ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö:

.. code-block:: python

   from evolution_langchain import EvolutionInference

   try:
       # –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä base_url
       llm = EvolutionInference(
           model="test-model",
           key_id="test-key",
           secret="test-secret"
           # base_url –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
       )
   except ValueError as e:
       print(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
       # –û—à–∏–±–∫–∞: base_url is required

–¢–∏–ø–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ ValueError:
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL
- –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

RuntimeError
~~~~~~~~~~~~

–í–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å API –∑–∞–ø—Ä–æ—Å–∞–º–∏:

.. code-block:: python

   try:
       llm = EvolutionInference(
           model="test-model",
           key_id="invalid-key",
           secret="invalid-secret",
           base_url="https://invalid-url.com/v1"
       )
       
       response = llm.invoke("–¢–µ—Å—Ç")
   except RuntimeError as e:
       print(f"–û—à–∏–±–∫–∞ API: {e}")
       # –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:
       # - –ù–µ–≤–µ—Ä–Ω—ã–µ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
       # - –ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞
       # - –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é

–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
-------------------------------

–ù–µ–≤–µ—Ä–Ω—ã–µ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import time
   from evolution_langchain import EvolutionInference

   def create_llm_with_retry(max_retries=3):
       """–°–æ–∑–¥–∞–Ω–∏–µ LLM —Å –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
       
       credentials = [
           ("primary-key", "primary-secret"),
           ("backup-key", "backup-secret"),
           ("fallback-key", "fallback-secret")
       ]
       
       for attempt in range(max_retries):
           try:
               key_id, secret = credentials[attempt % len(credentials)]
               
               llm = EvolutionInference(
                   model="your-model",
                   key_id=key_id,
                   secret=secret,
                   base_url="https://your-api-endpoint.com/v1"
               )
               
               # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
               llm.invoke("test")
               return llm
               
           except RuntimeError as e:
               print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
               if attempt < max_retries - 1:
                   time.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
               else:
                   raise Exception("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   try:
       llm = create_llm_with_retry()
       print("‚úÖ LLM —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")
   except Exception as e:
       print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LLM: {e}")

–ò—Å—Ç–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞
~~~~~~~~~~~~~~~~

.. code-block:: python

   def safe_invoke(llm, prompt, max_retries=3):
       """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å—Ç–µ—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞."""
       
       for attempt in range(max_retries):
           try:
               return llm.invoke(prompt)
               
           except RuntimeError as e:
               error_msg = str(e).lower()
               
               if "token" in error_msg or "401" in error_msg or "403" in error_msg:
                   print(f"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}: {e}")
                   
                   if attempt < max_retries - 1:
                       # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–æ—Å–∏—Ç—å —Ç–æ–∫–µ–Ω
                       if hasattr(llm, '_token_manager'):
                           llm._token_manager._token = None
                           llm._token_manager._token_expires_at = 0
                       
                       time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                       continue
                
               # –ù–µ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–æ–∫–µ–Ω–æ–º - –ø—Ä–æ–∫–∏–Ω—É—Ç—å –æ—à–∏–±–∫—É
               raise
       
       raise RuntimeError(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫: {max_retries}")

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   try:
       response = safe_invoke(llm, "–ü—Ä–∏–≤–µ—Ç!")
       print(response)
   except RuntimeError as e:
       print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")

–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
------------------------

–¢–∞–π–º–∞—É—Ç—ã –∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import requests
   from requests.exceptions import RequestException, Timeout, ConnectionError

   class RobustEvolutionInference:
       def __init__(self, **kwargs):
           self.config = kwargs
           self.llm = None
           self._initialize_llm()
       
       def _initialize_llm(self):
           """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏."""
           try:
               # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API endpoint
               base_url = self.config['base_url']
               test_url = base_url.replace('/v1', '/health')  # Endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è
               
               response = requests.get(test_url, timeout=5)
               if response.status_code != 200:
                   print(f"‚ö†Ô∏è  API endpoint –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
                
           except (ConnectionError, Timeout) as e:
               print(f"‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ API: {e}")
           except Exception as e:
               print(f"‚ö†Ô∏è  –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ API: {e}")
            
           # –°–æ–∑–¥–∞—Ç—å LLM –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
           self.llm = EvolutionInference(**self.config)
       
       def invoke(self, prompt, timeout=30):
           """–í—ã–∑–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
           try:
               # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–∞–π–º–∞—É—Ç —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
               old_timeout = self.llm.request_timeout
               self.llm.request_timeout = timeout
               
               response = self.llm.invoke(prompt)
               
               # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–∞–π–º–∞—É—Ç
               self.llm.request_timeout = old_timeout
               
               return response
               
           except RuntimeError as e:
               error_msg = str(e).lower()
               
               if "timeout" in error_msg:
                   raise TimeoutError(f"–ó–∞–ø—Ä–æ—Å –ø—Ä–µ–≤—ã—Å–∏–ª —Ç–∞–π–º–∞—É—Ç {timeout}—Å")
               elif "connection" in error_msg:
                   raise ConnectionError("–ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç–µ–≤—ã–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º")
               else:
                   raise e

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   robust_llm = RobustEvolutionInference(
       model="your-model",
       key_id="your-key-id",
       secret="your-secret",
       base_url="https://your-api-endpoint.com/v1"
   )

   try:
       response = robust_llm.invoke("–ü—Ä–∏–≤–µ—Ç!", timeout=10)
       print(response)
   except TimeoutError as e:
       print(f"–¢–∞–π–º–∞—É—Ç: {e}")
   except ConnectionError as e:
       print(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")

–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
--------------------------

–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   def handle_token_limit_error(llm, prompt, max_tokens=512):
       """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
       
       try:
           # –ü–æ–ø—ã—Ç–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
           response = llm.invoke(prompt)
           return response
           
       except RuntimeError as e:
           error_msg = str(e).lower()
           
           if "token" in error_msg and ("limit" in error_msg or "length" in error_msg):
               print("‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤, —É–º–µ–Ω—å—à–∞—é –ø—Ä–æ–º–ø—Ç...")
               
               # –£–º–µ–Ω—å—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç
               shortened_prompt = prompt[:len(prompt)//2] + "..."
               
               # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
               try:
                   response = llm.invoke(shortened_prompt)
                   return response + "\n\n[–û—Ç–≤–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π]"
               except RuntimeError as e2:
                   print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–∂–µ —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {e2}")
                   raise e2
           else:
               raise e

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   long_prompt = "–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç..." * 1000  # –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª–∏–Ω–Ω—ã–π
   
   try:
       response = handle_token_limit_error(llm, long_prompt)
       print(response)
   except RuntimeError as e:
       print(f"–û—à–∏–±–∫–∞: {e}")

–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   def validate_and_fix_parameters(llm, **kwargs):
       """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
       
       # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ temperature
       if 'temperature' in kwargs:
           temp = kwargs['temperature']
           if temp < 0:
               print(f"‚ö†Ô∏è  Temperature {temp} —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é 0")
               kwargs['temperature'] = 0
           elif temp > 2:
               print(f"‚ö†Ô∏è  Temperature {temp} —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é 2")
               kwargs['temperature'] = 2
       
       # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ max_tokens
       if 'max_tokens' in kwargs:
           max_tokens = kwargs['max_tokens']
           if max_tokens < 1:
               print(f"‚ö†Ô∏è  Max tokens {max_tokens} —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é 1")
               kwargs['max_tokens'] = 1
           elif max_tokens > 4000:
               print(f"‚ö†Ô∏è  Max tokens {max_tokens} —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é 4000")
               kwargs['max_tokens'] = 4000
       
       return kwargs

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   try:
       # –ü–æ–ø—ã—Ç–∫–∞ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
       response = llm.invoke("–ü—Ä–∏–≤–µ—Ç!", temperature=5.0, max_tokens=10000)
   except RuntimeError as e:
       print(f"–û—à–∏–±–∫–∞ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {e}")
       
       # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
       fixed_params = validate_and_fix_parameters(llm, temperature=5.0, max_tokens=10000)
       response = llm.invoke("–ü—Ä–∏–≤–µ—Ç!", **fixed_params)
       print(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {response}")

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
------------------

–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import logging
   import time
   from datetime import datetime

   # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
   logging.basicConfig(
       level=logging.INFO,
       format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
       handlers=[
           logging.FileHandler('evolution_errors.log'),
           logging.StreamHandler()
       ]
   )

   logger = logging.getLogger(__name__)

   class LoggedEvolutionInference:
       def __init__(self, llm):
           self.llm = llm
           self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
       
       def invoke(self, prompt):
           start_time = time.time()
           
           try:
               self.logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞: {prompt[:50]}...")
               response = self.llm.invoke(prompt)
               
               duration = time.time() - start_time
               self.logger.info(f"–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {duration:.2f}—Å")
               
               return response
               
           except Exception as e:
               duration = time.time() - start_time
               self.logger.error(
                   f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∑–∞ {duration:.2f}—Å: {type(e).__name__}: {e}"
               )
               raise

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   logged_llm = LoggedEvolutionInference(llm)

   try:
       response = logged_llm.invoke("–ü—Ä–∏–≤–µ—Ç!")
       print(response)
   except Exception as e:
       print(f"–û—à–∏–±–∫–∞: {e}")

–ú–µ—Ç—Ä–∏–∫–∏ –æ—à–∏–±–æ–∫
~~~~~~~~~~~~~~

.. code-block:: python

   from dataclasses import dataclass
   from typing import Dict, List
   import time

   @dataclass
   class ErrorMetric:
       error_type: str
       count: int = 0
       last_occurrence: str = ""
       avg_response_time: float = 0.0

   class ErrorTrackingEvolutionInference:
       def __init__(self, llm):
           self.llm = llm
           self.error_metrics: Dict[str, ErrorMetric] = {}
           self.total_requests = 0
           self.successful_requests = 0
       
       def invoke(self, prompt):
           start_time = time.time()
           self.total_requests += 1
           
           try:
               response = self.llm.invoke(prompt)
               self.successful_requests += 1
               return response
               
           except Exception as e:
               error_type = type(e).__name__
               duration = time.time() - start_time
               
               # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ—à–∏–±–æ–∫
               if error_type not in self.error_metrics:
                   self.error_metrics[error_type] = ErrorMetric(error_type)
               
               metric = self.error_metrics[error_type]
               metric.count += 1
               metric.last_occurrence = datetime.now().isoformat()
               
               # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
               if metric.count == 1:
                   metric.avg_response_time = duration
               else:
                   metric.avg_response_time = (
                       (metric.avg_response_time * (metric.count - 1) + duration) 
                       / metric.count
                   )
               
               raise e
       
       def get_error_report(self):
           """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–∞—Ö."""
           success_rate = (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0
           
           return {
               "total_requests": self.total_requests,
               "successful_requests": self.successful_requests,
               "success_rate": success_rate,
               "error_metrics": self.error_metrics
           }

   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
   tracked_llm = ErrorTrackingEvolutionInference(llm)

   # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
   for i in range(5):
       try:
           response = tracked_llm.invoke(f"–ó–∞–ø—Ä–æ—Å {i}")
           print(f"‚úÖ –ó–∞–ø—Ä–æ—Å {i} —É—Å–ø–µ—à–µ–Ω")
       except Exception as e:
           print(f"‚ùå –ó–∞–ø—Ä–æ—Å {i} –Ω–µ—É–¥–∞—á–µ–Ω: {e}")

   # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
   report = tracked_llm.get_error_report()
   print(f"–û—Ç—á–µ—Ç: {report}")

–ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
------------------------------

–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫:

.. code-block:: python

   import os
   import time
   import logging
   from dataclasses import dataclass
   from typing import Optional, Dict, Any
   from evolution_langchain import EvolutionInference

   @dataclass
   class ErrorConfig:
       max_retries: int = 3
       retry_delay: float = 1.0
       timeout: int = 30
       enable_logging: bool = True
       enable_metrics: bool = True

   class ComprehensiveErrorHandler:
       def __init__(self, llm, config: ErrorConfig):
           self.llm = llm
           self.config = config
           self.metrics = {
               "total_requests": 0,
               "successful_requests": 0,
               "errors": {}
           }
           
           if config.enable_logging:
               logging.basicConfig(level=logging.INFO)
               self.logger = logging.getLogger(__name__)
           else:
               self.logger = None
       
       def invoke(self, prompt: str, **kwargs) -> str:
           """–í—ã–∑–æ–≤ —Å –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
           self.metrics["total_requests"] += 1
           
           for attempt in range(self.config.max_retries):
               start_time = time.time()
               
               try:
                   # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∑–∞–ø—Ä–æ—Å–∞
                   if self.logger:
                       self.logger.info(f"–ó–∞–ø—Ä–æ—Å {attempt + 1}: {prompt[:50]}...")
                   
                   # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
                   response = self.llm.invoke(prompt, **kwargs)
                   
                   # –£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                   duration = time.time() - start_time
                   self.metrics["successful_requests"] += 1
                   
                   if self.logger:
                       self.logger.info(f"–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {duration:.2f}—Å")
                   
                   return response
                   
               except ValueError as e:
                   # –û—à–∏–±–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ - –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                   self._record_error("ValueError", e, duration=time.time() - start_time)
                   raise e
                   
               except RuntimeError as e:
                   duration = time.time() - start_time
                   error_msg = str(e).lower()
                   
                   # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ—à–∏–±–∫–∏
                   if "authentication" in error_msg or "401" in error_msg or "403" in error_msg:
                       error_type = "AuthenticationError"
                   elif "timeout" in error_msg:
                       error_type = "TimeoutError"
                   elif "connection" in error_msg:
                       error_type = "ConnectionError"
                   elif "token" in error_msg and "limit" in error_msg:
                       error_type = "TokenLimitError"
                   else:
                       error_type = "RuntimeError"
                   
                   self._record_error(error_type, e, duration)
                   
                   # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
                   if self.logger:
                       self.logger.warning(
                           f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ—É–¥–∞—á–Ω–∞: {error_type}: {e}"
                       )
                   
                   # –†–µ—à–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –æ—à–∏–±–∫–∏
                   if error_type == "TokenLimitError":
                       # –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
                       shortened_prompt = prompt[:len(prompt)//2] + "..."
                       if self.logger:
                           self.logger.info("–°–æ–∫—Ä–∞—â–∞—é –ø—Ä–æ–º–ø—Ç –∏–∑-–∑–∞ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤")
                       prompt = shortened_prompt
                       continue
                   
                   elif error_type == "AuthenticationError":
                       # –°–±—Ä–æ—Å —Ç–æ–∫–µ–Ω–∞
                       if hasattr(self.llm, '_token_manager'):
                           self.llm._token_manager._token = None
                           self.llm._token_manager._token_expires_at = 0
                       if self.logger:
                           self.logger.info("–°–±—Ä–∞—Å—ã–≤–∞—é —Ç–æ–∫–µ–Ω –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                   
                   # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
                   if attempt < self.config.max_retries - 1:
                       delay = self.config.retry_delay * (2 ** attempt)
                       if self.logger:
                           self.logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {delay}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π")
                       time.sleep(delay)
                       continue
                   
                   # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
                   if self.logger:
                       self.logger.error(f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã: {e}")
                   raise e
                   
               except Exception as e:
                   # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
                   duration = time.time() - start_time
                   self._record_error("UnexpectedError", e, duration)
                   
                   if self.logger:
                       self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__}: {e}")
                   raise e
           
           raise RuntimeError("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
       
       def _record_error(self, error_type: str, error: Exception, duration: float):
           """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫ –æ—à–∏–±–∫–∏."""
           if not self.config.enable_metrics:
               return
           
           if error_type not in self.metrics["errors"]:
               self.metrics["errors"][error_type] = {
                   "count": 0,
                   "last_occurrence": "",
                   "avg_duration": 0.0
               }
           
           error_metric = self.metrics["errors"][error_type]
           error_metric["count"] += 1
           error_metric["last_occurrence"] = time.strftime("%Y-%m-%d %H:%M:%S")
           
           # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
           if error_metric["count"] == 1:
               error_metric["avg_duration"] = duration
           else:
               error_metric["avg_duration"] = (
                   (error_metric["avg_duration"] * (error_metric["count"] - 1) + duration) 
                   / error_metric["count"]
               )
       
       def get_metrics(self) -> Dict[str, Any]:
           """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫."""
           if not self.config.enable_metrics:
               return {}
           
           success_rate = (
               (self.metrics["successful_requests"] / self.metrics["total_requests"] * 100) 
               if self.metrics["total_requests"] > 0 else 0
           )
           
           return {
               "total_requests": self.metrics["total_requests"],
               "successful_requests": self.metrics["successful_requests"],
               "success_rate": success_rate,
               "errors": self.metrics["errors"]
           }

   def main():
       print("üöÄ Evolution LangChain - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫")
       print("=" * 50)

       # –°–æ–∑–¥–∞–Ω–∏–µ LLM
       llm = EvolutionInference(
           model=os.getenv("EVOLUTION_MODEL", "your-model"),
           key_id=os.getenv("EVOLUTION_KEY_ID", "your-key-id"),
           secret=os.getenv("EVOLUTION_SECRET", "your-secret"),
           base_url=os.getenv("EVOLUTION_BASE_URL", "https://your-api-endpoint.com/v1")
       )

       # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫
       config = ErrorConfig(
           max_retries=3,
           retry_delay=1.0,
           timeout=30,
           enable_logging=True,
           enable_metrics=True
       )

       error_handler = ComprehensiveErrorHandler(llm, config)

       print("‚úÖ –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
       print()

       # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
       test_cases = [
           "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
           "–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏",
           "–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç..." * 1000,  # –¢–µ—Å—Ç –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
           "–¢–µ—Å—Ç —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"
       ]

       for i, test_case in enumerate(test_cases, 1):
           print(f"–¢–µ—Å—Ç {i}: {test_case[:50]}...")
           
           try:
               if i == 4:  # –¢–µ—Å—Ç —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                   response = error_handler.invoke(test_case, temperature=5.0)
               else:
                   response = error_handler.invoke(test_case)
               
               print(f"‚úÖ –£—Å–ø–µ—Ö: {response[:100]}...")
               
           except Exception as e:
               print(f"‚ùå –û—à–∏–±–∫–∞: {type(e).__name__}: {e}")
           
           print()

       # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
       metrics = error_handler.get_metrics()
       print("üìä –ú–µ—Ç—Ä–∏–∫–∏:")
       for key, value in metrics.items():
           if key == "errors":
               print(f"  {key}:")
               for error_type, error_data in value.items():
                   print(f"    {error_type}: {error_data}")
           else:
               print(f"  {key}: {value}")

   if __name__ == "__main__":
       main()

–ß—Ç–æ –¥–∞–ª—å—à–µ?
-----------

- –ò–∑—É—á–∏—Ç–µ :doc:`../guide/token-management` –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞–º–∏
- –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ :doc:`../guide/langchain-integration` –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å LangChain
- –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ :doc:`../api/evolution-inference` –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ API 