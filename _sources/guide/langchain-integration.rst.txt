–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangChain
======================

–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Evolution LangChain —Å —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π LangChain.

–û—Å–Ω–æ–≤—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
-----------------

BaseLLM –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
~~~~~~~~~~~~~~~~~~~~

``EvolutionInference`` –Ω–∞—Å–ª–µ–¥—É–µ—Ç—Å—è –æ—Ç ``BaseLLM`` –∏ —Ä–µ–∞–ª–∏–∑—É–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–µ—Ç–æ–¥—ã:

.. code-block:: python

    from evolution_langchain import EvolutionInference
    from langchain_core.language_models.base import BaseLLM

    # EvolutionInference —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º LangChain LLM
    llm = EvolutionInference(
        model="your-model",
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1"
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    assert isinstance(llm, BaseLLM)
    print(f"LLM —Ç–∏–ø: {llm._llm_type}")  # "cloud-ru-tech"

–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã LangChain
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    response = llm.invoke("–ü—Ä–∏–≤–µ—Ç!")
    responses = llm.generate(["–í–æ–ø—Ä–æ—Å 1", "–í–æ–ø—Ä–æ—Å 2"])

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã  
    response = await llm.ainvoke("–ü—Ä–∏–≤–µ—Ç!")
    responses = await llm.agenerate(["–í–æ–ø—Ä–æ—Å 1", "–í–æ–ø—Ä–æ—Å 2"])

    # Streaming (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    for chunk in llm.stream("–†–∞—Å—Å–∫–∞–∂–∏ –∏—Å—Ç–æ—Ä–∏—é"):
        print(chunk, end="")

–¶–µ–ø–æ—á–∫–∏ (Chains)
----------------

LLMChain
~~~~~~~~

–ë–∞–∑–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º:

.. code-block:: python

    from langchain.chains import LLMChain
    from langchain.prompts import PromptTemplate

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
    template = """
    –¢—ã - –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å:

    –í–æ–ø—Ä–æ—Å: {question}

    –û—Ç–≤–µ—Ç:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["question"]
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏
    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        verbose=True
    )

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    result = chain.run("–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞–º—ã–∫–∞–Ω–∏–µ –≤ JavaScript?")
    print(result)

SimpleSequentialChain
~~~~~~~~~~~~~~~~~~~~~

–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫:

.. code-block:: python

    from langchain.chains import SimpleSequentialChain

    # –ü–µ—Ä–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–¥–µ–∏
    idea_template = "–ü—Ä–∏–¥—É–º–∞–π –∏–¥–µ—é –¥–ª—è {topic}"
    idea_prompt = PromptTemplate(template=idea_template, input_variables=["topic"])
    idea_chain = LLMChain(llm=llm, prompt=idea_prompt)

    # –í—Ç–æ—Ä–∞—è —Ü–µ–ø–æ—á–∫–∞ - —Ä–∞–∑–≤–∏—Ç–∏–µ –∏–¥–µ–∏
    develop_template = "–†–∞–∑–≤–µ–π —ç—Ç—É –∏–¥–µ—é –ø–æ–¥—Ä–æ–±–Ω–µ–µ: {idea}"
    develop_prompt = PromptTemplate(template=develop_template, input_variables=["idea"])
    develop_chain = LLMChain(llm=llm, prompt=develop_prompt)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é —Ü–µ–ø–æ—á–∫—É
    overall_chain = SimpleSequentialChain(
        chains=[idea_chain, develop_chain],
        verbose=True
    )

    result = overall_chain.run("–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ")

ConversationChain
~~~~~~~~~~~~~~~~~

–î–∏–∞–ª–æ–≥–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ —Å –ø–∞–º—è—Ç—å—é:

.. code-block:: python

    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
    memory = ConversationBufferMemory()

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤–æ–π —Ü–µ–ø–æ—á–∫–∏
    conversation = ConversationChain(
        llm=llm,
        memory=memory,
        verbose=True
    )

    # –î–∏–∞–ª–æ–≥
    response1 = conversation.predict(input="–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π.")
    print(response1)

    response2 = conversation.predict(input="–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?")
    print(response2)  # –î–æ–ª–∂–µ–Ω –ø–æ–º–Ω–∏—Ç—å –∏–º—è

–ê–≥–µ–Ω—Ç—ã (Agents)
---------------

ReAct –∞–≥–µ–Ω—Ç
~~~~~~~~~~~

–ê–≥–µ–Ω—Ç —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á:

.. code-block:: python

    from langchain.agents import initialize_agent, Tool, AgentType
    from langchain.tools import DuckDuckGoSearchRun

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    search = DuckDuckGoSearchRun()

    tools = [
        Tool(
            name="Search",
            func=search.run,
            description="–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"
        ),
        Tool(
            name="Calculator",
            func=lambda x: str(eval(x)),
            description="–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"
        )
    ]

    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True
    )

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
    result = agent.run("–ù–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Python –∏ –≤—ã—á–∏—Å–ª–∏ 15 * 23")
    print(result)

–ö–∞—Å—Ç–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç
~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.agents import BaseSingleActionAgent
    from langchain.schema import AgentAction, AgentFinish

    class CustomAgent(BaseSingleActionAgent):
        def plan(self, intermediate_steps, **kwargs):
            # –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞
            user_input = kwargs.get("input", "")
            
            if "–ø–æ–∏—Å–∫" in user_input.lower():
                return AgentAction(
                    tool="Search",
                    tool_input=user_input,
                    log="–í—ã–ø–æ–ª–Ω—è—é –ø–æ–∏—Å–∫"
                )
            else:
                return AgentFinish(
                    return_values={"output": f"–û—Ç–≤–µ—Ç –Ω–∞: {user_input}"},
                    log="–ó–∞–≤–µ—Ä—à–∞—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ"
                )
        
        @property
        def input_keys(self):
            return ["input"]

        @property
        def output_keys(self):
            return ["output"]

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
    agent = CustomAgent()
    agent_executor = AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=tools,
        verbose=True
    )

–í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
-------------------

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.vectorstores import FAISS
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.text_splitter import CharacterTextSplitter
    from langchain.chains import RetrievalQA

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    documents = [
        "Python - —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è",
        "JavaScript –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏",
        "Machine Learning - –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
    ]

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
    texts = text_splitter.create_documents(documents)

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    embeddings = HuggingFaceEmbeddings()
    vectorstore = FAISS.from_documents(texts, embeddings)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        verbose=True
    )

    # –ó–∞–ø—Ä–æ—Å
    result = qa_chain.run("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
    print(result)

RAG (Retrieval-Augmented Generation)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate

    # –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è RAG
    template = """
    –ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å:

    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

    –í–æ–ø—Ä–æ—Å: {question}

    –û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    # RAG —Ü–µ–ø–æ—á–∫–∞ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
    rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": prompt},
        verbose=True
    )

    result = rag_chain.run("–û–±—ä—è—Å–Ω–∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")

–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
--------------------

–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.chains.summarize import load_summarize_chain

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    loader = TextLoader("document.txt")
    documents = loader.load()

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    texts = text_splitter.split_documents(documents)

    # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
    summarize_chain = load_summarize_chain(
        llm=llm,
        chain_type="map_reduce",
        verbose=True
    )

    summary = summarize_chain.run(texts)
    print(summary)

–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
~~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.chains import AnalyzeDocumentChain
    from langchain.chains.question_answering import load_qa_chain

    # –¶–µ–ø–æ—á–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    qa_chain = load_qa_chain(llm=llm, chain_type="stuff")
    analyze_chain = AnalyzeDocumentChain(
        combine_docs_chain=qa_chain,
        text_splitter=text_splitter
    )

    # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    with open("document.txt") as f:
        content = f.read()

    result = analyze_chain.run(
        input_document=content,
        question="–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?"
    )

–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ LangChain
-----------------------

–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ LLM
~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.cache import InMemoryCache
    from langchain.globals import set_llm_cache

    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫—ç—à–∞
    set_llm_cache(InMemoryCache())

    # –¢–µ–ø–µ—Ä—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å—Å—è
    response1 = llm.invoke("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
    response2 = llm.invoke("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")  # –ò–∑ –∫—ç—à–∞

SQLite –∫—ç—à
~~~~~~~~~~

.. code-block:: python

    from langchain.cache import SQLiteCache

    set_llm_cache(SQLiteCache(database_path=".langchain.db"))

    # –ö—ç—à –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏

Callbacks –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
----------------------

–ö–∞—Å—Ç–æ–º–Ω—ã–µ callback'–∏
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.callbacks.base import BaseCallbackHandler
    from langchain.schema import LLMResult

    class CustomCallbackHandler(BaseCallbackHandler):
        def on_llm_start(self, serialized, prompts, **kwargs):
            print(f"LLM –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É —Å {len(prompts)} –ø—Ä–æ–º–ø—Ç–∞–º–∏")
        
        def on_llm_end(self, response: LLMResult, **kwargs):
            print(f"LLM –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª {len(response.generations)} –æ—Ç–≤–µ—Ç–æ–≤")
        
        def on_llm_error(self, error, **kwargs):
            print(f"–û—à–∏–±–∫–∞ LLM: {error}")

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ callback'–∞
    callback_handler = CustomCallbackHandler()

    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        callbacks=[callback_handler]
    )

    result = chain.run("–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangSmith
~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    import os
    from langchain.callbacks.tracers import LangChainTracer

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LangSmith (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-key"

    # –¢—Ä–µ–π—Å–∏–Ω–≥ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º
    tracer = LangChainTracer()

    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        callbacks=[tracer]
    )

–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
--------------------

–£—Å–ª–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.chains import LLMChain
    from langchain.chains.base import Chain

    class ConditionalChain(Chain):
        def __init__(self, condition_chain, true_chain, false_chain):
            self.condition_chain = condition_chain
            self.true_chain = true_chain
            self.false_chain = false_chain
        
        @property
        def input_keys(self):
            return ["input"]
        
        @property  
        def output_keys(self):
            return ["output"]
        
        def _call(self, inputs):
            condition_result = self.condition_chain.run(inputs["input"])
            
            if "–¥–∞" in condition_result.lower():
                result = self.true_chain.run(inputs["input"])
            else:
                result = self.false_chain.run(inputs["input"])
            
            return {"output": result}

    # –°–æ–∑–¥–∞–Ω–∏–µ —É—Å–ª–æ–≤–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫
    condition_prompt = PromptTemplate(
        template="–≠—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏? –û—Ç–≤–µ—Ç—å '–¥–∞' –∏–ª–∏ '–Ω–µ—Ç': {input}",
        input_variables=["input"]
    )

    programming_prompt = PromptTemplate(
        template="–û—Ç–≤–µ—Ç—å –∫–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é: {input}",
        input_variables=["input"]
    )

    general_prompt = PromptTemplate(
        template="–û—Ç–≤–µ—Ç—å –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫: {input}",
        input_variables=["input"]
    )

    conditional_chain = ConditionalChain(
        condition_chain=LLMChain(llm=llm, prompt=condition_prompt),
        true_chain=LLMChain(llm=llm, prompt=programming_prompt),
        false_chain=LLMChain(llm=llm, prompt=general_prompt)
    )

–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
---------------

–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
~~~~~~~~~~~~~~~~

.. code-block:: python

    from langchain.schema import OutputParserException

    try:
        result = chain.run("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–≤–æ–¥")
    except OutputParserException as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        # –ü–æ–≤—Ç–æ—Ä–∏—Ç—å —Å –¥—Ä—É–≥–∏–º –ø—Ä–æ–º–ø—Ç–æ–º –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç–æ–º
    except Exception as e:
        print(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞

–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    # –î–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    creative_llm = EvolutionInference(
        model="your-model",
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1",
        temperature=1.2,  # –ë–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ
        max_tokens=1000
    )

    precise_llm = EvolutionInference(
        model="your-model", 
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1",
        temperature=0.1,  # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ
        max_tokens=200
    )

–ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
~~~~~~~~~~~~~~~~~

.. code-block:: python

    # –°–æ–∑–¥–∞–π—Ç–µ –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    base_config = {
        "model": "your-model",
        "key_id": os.getenv("EVOLUTION_KEY_ID"),
        "secret": os.getenv("EVOLUTION_SECRET"),
        "base_url": os.getenv("EVOLUTION_BASE_URL")
    }

    # –°–æ–∑–¥–∞–≤–∞–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã
    chat_llm = EvolutionInference(**base_config, temperature=0.7, max_tokens=500)
    analysis_llm = EvolutionInference(**base_config, temperature=0.1, max_tokens=1000)
    creative_llm = EvolutionInference(**base_config, temperature=1.5, max_tokens=2000)

–ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
------------------------

.. code-block:: python

    import os
    from evolution_langchain import EvolutionInference
    from langchain.chains import LLMChain, ConversationChain
    from langchain.prompts import PromptTemplate
    from langchain.memory import ConversationBufferMemory
    from langchain.agents import initialize_agent, Tool, AgentType
    from langchain.tools import DuckDuckGoSearchRun

    def main():
        print("üöÄ Evolution LangChain - –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è")
        print("=" * 50)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        llm = EvolutionInference(
            model=os.getenv("EVOLUTION_MODEL", "your-model"),
            key_id=os.getenv("EVOLUTION_KEY_ID", "your-key-id"),
            secret=os.getenv("EVOLUTION_SECRET", "your-secret"),
            base_url=os.getenv("EVOLUTION_BASE_URL", "https://your-api-endpoint.com/v1")
        )

        print("‚úÖ LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print()

        # 1. –ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ø–æ—á–∫–∞
        print("1. –ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ø–æ—á–∫–∞:")
        template = "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}"
        prompt = PromptTemplate(template=template, input_variables=["question"])
        chain = LLMChain(llm=llm, prompt=prompt)

        try:
            result = chain.run("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
            print(f"–û—Ç–≤–µ—Ç: {result}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print()

        # 2. –î–∏–∞–ª–æ–≥–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞
        print("2. –î–∏–∞–ª–æ–≥–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞:")
        memory = ConversationBufferMemory()
        conversation = ConversationChain(llm=llm, memory=memory, verbose=False)

        try:
            conversation.predict(input="–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–Ω–Ω–∞.")
            response = conversation.predict(input="–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?")
            print(f"–û—Ç–≤–µ—Ç: {response}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print()

        # 3. –ê–≥–µ–Ω—Ç —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        print("3. –ê–≥–µ–Ω—Ç —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏:")
        search = DuckDuckGoSearchRun()
        tools = [
            Tool(
                name="Search",
                func=search.run,
                description="–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"
            )
        ]

        agent = initialize_agent(
            tools=tools,
            llm=llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=False
        )

        try:
            result = agent.run("–ù–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Python")
            print(f"–û—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞: {result}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print()

        print("üéâ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    if __name__ == "__main__":
        main()

–ß—Ç–æ –¥–∞–ª—å—à–µ?
-----------

- –ò–∑—É—á–∏—Ç–µ :doc:`basics` –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ :doc:`token-management` –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞–º–∏
- –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ :doc:`error-handling` –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
- –ò–∑—É—á–∏—Ç–µ :doc:`../examples/advanced` –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ 