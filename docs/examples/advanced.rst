–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã
===================

–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Evolution LangChain –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.

RAG —Å–∏—Å—Ç–µ–º–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
-------------------------

–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

.. code-block:: python

    from evolution_langchain import EvolutionInference
    from langchain.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.vectorstores import FAISS
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
    llm = EvolutionInference(
        model="your-model",
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1",
        temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        max_tokens=800
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    def create_rag_system(document_paths):
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = []
        for path in document_paths:
            loader = TextLoader(path, encoding='utf-8')
            documents.extend(loader.load())
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        texts = text_splitter.split_documents(documents)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        vectorstore = FAISS.from_documents(texts, embeddings)
        
        # –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        template = """
        –ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}
        
        –í–æ–ø—Ä–æ—Å: {question}
        
        –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç:"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ RAG —Ü–µ–ø–æ—á–∫–∏
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
        
        return qa_chain

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    document_paths = ["doc1.txt", "doc2.txt", "doc3.txt"]
    rag_system = create_rag_system(document_paths)

    query = "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Python?"
    result = rag_system({"query": query})

    print(f"–í–æ–ø—Ä–æ—Å: {query}")
    print(f"–û—Ç–≤–µ—Ç: {result['result']}")
    print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {[doc.metadata for doc in result['source_documents']]}")

–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
----------------------

–°–∏—Å—Ç–µ–º–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤:

.. code-block:: python

    from langchain.agents import initialize_agent, Tool, AgentType
    from langchain.memory import ConversationBufferMemory
    from langchain.schema import BaseOutputParser
    import requests

    class SpecializedAgent:
        def __init__(self, name, system_prompt, llm):
            self.name = name
            self.system_prompt = system_prompt
            self.llm = llm
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
            from langchain.prompts import PromptTemplate
            
            template = f"""
            {system_prompt}
            
            –ß–µ–ª–æ–≤–µ–∫: {{input}}
            –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"""
            
            self.prompt = PromptTemplate(
                template=template,
                input_variables=["input"]
            )
        
        def process(self, input_text):
            formatted_prompt = self.prompt.format(input=input_text)
            return self.llm.invoke(formatted_prompt)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
    code_agent = SpecializedAgent(
        name="CodeExpert",
        system_prompt="–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–¥–µ, –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö.",
        llm=llm
    )

    math_agent = SpecializedAgent(
        name="MathExpert", 
        system_prompt="–¢—ã - –º–∞—Ç–µ–º–∞—Ç–∏–∫. –†–µ—à–∞–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –æ–±—ä—è—Å–Ω—è–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏.",
        llm=llm
    )

    general_agent = SpecializedAgent(
        name="GeneralAssistant",
        system_prompt="–¢—ã - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã.",
        llm=llm
    )

    class AgentRouter:
        def __init__(self, agents, classifier_llm):
            self.agents = agents
            self.classifier = classifier_llm
        
        def route(self, query):
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
            classification_prompt = f"""
            –û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞:
            
            –í–æ–ø—Ä–æ—Å: {query}
            
            –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:
            - code: –≤–æ–ø—Ä–æ—Å—ã –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –∫–æ–¥–µ, –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö
            - math: –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –≤–æ–ø—Ä–æ—Å—ã
            - general: –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            
            –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:"""
            
            category = self.classifier.invoke(classification_prompt).strip().lower()
            
            # –í—ã–±–æ—Ä –∞–≥–µ–Ω—Ç–∞
            if "code" in category:
                return self.agents["code"]
            elif "math" in category:
                return self.agents["math"]
            else:
                return self.agents["general"]
        
        def process(self, query):
            selected_agent = self.route(query)
            print(f"üéØ –í—ã–±—Ä–∞–Ω –∞–≥–µ–Ω—Ç: {selected_agent.name}")
            return selected_agent.process(query)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–∞
    agents = {
        "code": code_agent,
        "math": math_agent,
        "general": general_agent
    }

    router = AgentRouter(agents, llm)

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    queries = [
        "–ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—É—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –Ω–∞ Python?",
        "–í—ã—á–∏—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ f(x) = x^2 + 3x + 2",
        "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ?"
    ]

    for query in queries:
        print(f"\nüìù –ó–∞–ø—Ä–æ—Å: {query}")
        response = router.process(query)
        print(f"üí¨ –û—Ç–≤–µ—Ç: {response}")

–°–∏—Å—Ç–µ–º–∞ —Å –ø–∞–º—è—Ç—å—é –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
------------------------------

–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç—å—é:

.. code-block:: python

    from evolution_langchain import EvolutionInference
    from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
    from langchain.chains import ConversationChain
    from langchain.prompts import PromptTemplate

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
    llm = EvolutionInference(
        model="your-model",
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1"
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ —Å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π
    memory = ConversationSummaryMemory(llm=llm)

    # –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
    template = """
    –¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –ø–∞–º—è—Ç—å—é –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–∞—Ö.
    
    –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:
    {history}
    
    –ß–µ–ª–æ–≤–µ–∫: {input}
    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["history", "input"]
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —Å –ø–∞–º—è—Ç—å—é
    conversation = ConversationChain(
        llm=llm,
        memory=memory,
        prompt=prompt,
        verbose=True
    )

    # –î–∏–∞–ª–æ–≥
    responses = []
    messages = [
        "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π.",
        "–Ø –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –∏ –∏–∑—É—á–∞—é Python.",
        "–ö–∞–∫–∏–µ –∫–Ω–∏–≥–∏ –ø–æ Python —Ç—ã –º–æ–∂–µ—à—å –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å?",
        "–ê —á—Ç–æ –Ω–∞—Å—á–µ—Ç –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è?",
        "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç –∏ —á–µ–º —è –∑–∞–Ω–∏–º–∞—é—Å—å?"
    ]

    for message in messages:
        response = conversation.predict(input=message)
        responses.append(response)
        print(f"–ß–µ–ª–æ–≤–µ–∫: {message}")
        print(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {response}")
        print("-" * 50)

–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
---------------------------------

–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º:

.. code-block:: python

    import time
    import logging
    from dataclasses import dataclass
    from typing import List, Optional
    from evolution_langchain import EvolutionInference

    @dataclass
    class RequestMetrics:
        prompt: str
        response: str
        duration: float
        tokens_used: Optional[int] = None
        success: bool = True
        error: Optional[str] = None

    class MonitoredEvolutionInference:
        def __init__(self, llm):
            self.llm = llm
            self.metrics: List[RequestMetrics] = []
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger(__name__)
        
        def invoke(self, prompt):
            start_time = time.time()
            
            try:
                response = self.llm.invoke(prompt)
                duration = time.time() - start_time
                
                # –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
                estimated_tokens = (len(prompt) + len(response)) // 4
                
                metric = RequestMetrics(
                    prompt=prompt[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                    response=response[:100],
                    duration=duration,
                    tokens_used=estimated_tokens,
                    success=True
                )
                
                self.metrics.append(metric)
                self.logger.info(f"–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {duration:.2f}—Å")
                return response
                
            except Exception as e:
                duration = time.time() - start_time
                
                metric = RequestMetrics(
                    prompt=prompt[:100],
                    response="",
                    duration=duration,
                    success=False,
                    error=str(e)
                )
                
                self.metrics.append(metric)
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∑–∞ {duration:.2f}—Å: {e}")
                raise
        
        def get_stats(self):
            """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
            if not self.metrics:
                return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
            
            total_requests = len(self.metrics)
            successful_requests = sum(1 for m in self.metrics if m.success)
            avg_duration = sum(m.duration for m in self.metrics) / total_requests
            total_tokens = sum(m.tokens_used or 0 for m in self.metrics)
            
            return {
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "success_rate": successful_requests / total_requests * 100,
                "avg_duration": avg_duration,
                "total_estimated_tokens": total_tokens
            }

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    llm = EvolutionInference(
        model="your-model",
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1"
    )

    monitored_llm = MonitoredEvolutionInference(llm)

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
    questions = ["–ß—Ç–æ —Ç–∞–∫–æ–µ –ò–ò?", "–ß—Ç–æ —Ç–∞–∫–æ–µ ML?", "–ß—Ç–æ —Ç–∞–∫–æ–µ DL?"]
    
    for question in questions:
        try:
            response = monitored_llm.invoke(question)
            print(f"Q: {question}")
            print(f"A: {response[:100]}...")
            print("-" * 50)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ '{question}': {e}")

    # –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = monitored_llm.get_stats()
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")

–°–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
-------------------

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è:

.. code-block:: python

    import hashlib
    import json
    import os
    from evolution_langchain import EvolutionInference

    class CacheManager:
        def __init__(self, cache_dir="llm_cache"):
            self.cache_dir = cache_dir
            os.makedirs(cache_dir, exist_ok=True)
        
        def _get_cache_key(self, prompt, **kwargs):
            """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
            cache_data = {"prompt": prompt, **kwargs}
            cache_string = json.dumps(cache_data, sort_keys=True)
            return hashlib.md5(cache_string.encode()).hexdigest()
        
        def get(self, prompt, **kwargs):
            """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞."""
            key = self._get_cache_key(prompt, **kwargs)
            cache_file = os.path.join(self.cache_dir, f"{key}.json")
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)["response"]
            return None
        
        def set(self, prompt, response, **kwargs):
            """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à."""
            key = self._get_cache_key(prompt, **kwargs)
            cache_file = os.path.join(self.cache_dir, f"{key}.json")
            
            cache_data = {
                "prompt": prompt,
                "response": response,
                "params": kwargs
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

    class CachedEvolutionInference:
        def __init__(self, llm, cache_manager):
            self.llm = llm
            self.cache = cache_manager
        
        def invoke(self, prompt, **kwargs):
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
            cached_response = self.cache.get(prompt, **kwargs)
            if cached_response:
                print("üì¶ –û—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞")
                return cached_response
            
            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å
            print("üåê –ó–∞–ø—Ä–æ—Å –∫ API")
            response = self.llm.invoke(prompt)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à
            self.cache.set(prompt, response, **kwargs)
            
            return response

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    llm = EvolutionInference(
        model="your-model",
        key_id="your-key-id",
        secret="your-secret",
        base_url="https://your-api-endpoint.com/v1"
    )

    cache_manager = CacheManager()
    cached_llm = CachedEvolutionInference(llm, cache_manager)

    # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å - –∫ API
    response1 = cached_llm.invoke("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
    print(response1)

    # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å - –∏–∑ –∫—ç—à–∞
    response2 = cached_llm.invoke("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?")
    print(response2)

–ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä
-------------

–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:

.. code-block:: python

    import os
    from evolution_langchain import EvolutionInference
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationSummaryMemory

    def main():
        print("üöÄ Evolution LangChain - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã")
        print("=" * 60)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        llm = EvolutionInference(
            model=os.getenv("EVOLUTION_MODEL", "your-model-name"),
            key_id=os.getenv("EVOLUTION_KEY_ID", "your-key-id"),
            secret=os.getenv("EVOLUTION_SECRET", "your-secret"),
            base_url=os.getenv("EVOLUTION_BASE_URL", "https://your-api-endpoint.com/v1")
        )

        print("‚úÖ LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print()

        # 1. –°–∏—Å—Ç–µ–º–∞ —Å –ø–∞–º—è—Ç—å—é
        print("1. –°–∏—Å—Ç–µ–º–∞ —Å –ø–∞–º—è—Ç—å—é:")
        memory = ConversationSummaryMemory(llm=llm)
        conversation = ConversationChain(
            llm=llm,
            memory=memory,
            verbose=False
        )

        try:
            conversation.predict(input="–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω.")
            conversation.predict(input="–Ø –∏–∑—É—á–∞—é –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.")
            response = conversation.predict(input="–ß—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å –æ–±–æ –º–Ω–µ?")
            print(f"–û—Ç–≤–µ—Ç: {response}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print()

        # 2. –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
        print("2. –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞:")
        from langchain.prompts import PromptTemplate

        class SimpleAgent:
            def __init__(self, name, system_prompt, llm):
                self.name = name
                template = f"{system_prompt}\n\n–í–æ–ø—Ä–æ—Å: {{input}}\n–û—Ç–≤–µ—Ç:"
                self.prompt = PromptTemplate(template=template, input_variables=["input"])
                self.llm = llm
            
            def process(self, input_text):
                formatted_prompt = self.prompt.format(input=input_text)
                return self.llm.invoke(formatted_prompt)

        code_agent = SimpleAgent(
            "CodeExpert",
            "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.",
            llm
        )

        try:
            response = code_agent.process("–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤ Python?")
            print(f"–û—Ç–≤–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞ –ø–æ –∫–æ–¥—É: {response}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print()

        # 3. –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        print("3. –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:")
        import time

        class SimpleMonitor:
            def __init__(self, llm):
                self.llm = llm
                self.requests = 0
                self.total_time = 0
            
            def invoke(self, prompt):
                start_time = time.time()
                try:
                    response = self.llm.invoke(prompt)
                    duration = time.time() - start_time
                    self.requests += 1
                    self.total_time += duration
                    print(f"‚úÖ –ó–∞–ø—Ä–æ—Å {self.requests} –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {duration:.2f}—Å")
                    return response
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                    raise

        monitored_llm = SimpleMonitor(llm)

        try:
            monitored_llm.invoke("–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç")
            monitored_llm.invoke("–ß—Ç–æ —Ç–∞–∫–æ–µ –ò–ò?")
            print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {monitored_llm.total_time/monitored_llm.requests:.2f}—Å")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print()

        print("üéâ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")

    if __name__ == "__main__":
        main()

–ß—Ç–æ –¥–∞–ª—å—à–µ?
-----------

- –ò–∑—É—á–∏—Ç–µ :doc:`../guide/langchain-integration` –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
- –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ :doc:`../guide/token-management` –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞–º–∏
- –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ :doc:`../api/evolution-inference` –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ API 